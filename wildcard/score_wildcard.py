from scrape_wildcard import WildcardParser

prefix_q = """
Here is a scoring assessment that I want you to utilize: 
--- Each metric would be scored on a scale from 0 to 1, where 0 represents the lowest performance and 1 represents the highest. Here are the metrics I would use:
Relevance (0-1): Measures how well the response addresses the given prompt. A score of 1 would indicate perfect alignment with the prompt, while a score of 0 would mean the response is completely off-topic.
Clarity (0-1): Assesses the ease of understanding the response. A score of 1 would be given for clear, concise, and easily understandable language, while a score of 0 would be for a response that is confusing or difficult to comprehend.
Accuracy (0-1): Evaluates the factual correctness of the response. A score of 1 indicates that the information provided is entirely accurate, and 0 would mean the response contains incorrect or misleading information.
Creativity (0-1): Judges the originality and imaginative aspects of the response. A score of 1 is for highly creative and unique content, while 0 is for generic or clich√©d responses.
Cohesion (0-1): Looks at the logical flow and structure of the response. A score of 1 is for a well-structured response with a logical flow, while 0 is for disjointed or poorly organized content. ---
To provide an example of how these metrics would be applied, let's consider a hypothetical response to a prompt asking for a brief summary of the novel "1984" by George Orwell:
Hypothetical Response: "1984 is a science fiction novel. It talks about technology and has interesting characters. The story is set in the future and explores themes of freedom and control."
Relevance: 0.7 - The response is somewhat relevant but lacks specific details about the novel.
Clarity: 0.8 - The response is clear but could be more detailed.
Accuracy: 0.5 - The response correctly identifies the genre but misses key aspects of the plot and themes.
Creativity: 0.4 - The response is straightforward with little creative interpretation or insight.
Cohesion: 0.6 - The response is somewhat organized but lacks a cohesive summary of the novel.
Using these scores, the overall score for this hypothetical response would be the average: (0.7 + 0.8 + 0.5 + 0.4 + 0.6) / 5 = 0.6

I would like you to grade the following response using this set of metrics. I will give you the prompt first, then the response generated by the LLM in response to the prompt. Please give your score as an average of the above metrics. Also, please write your answer in the format: "The correct answer is: _" where the underscore represents your score.
"""

'''
Pass questions into LLaMA v2 to derive outputs
'''

class InteractLLaMA:
	def __init__(self):
		self.model, self.tokenizer = self.load_llama_model()
			
	def generate_message(self, question):
		formatted_question = question.replace("'", '')
		return ([{'role': 'system', 'content': ''}, 
				 {'role': 'user', 'content': (formatted_question)}])

	def messages_to_prompt(self, messages):
		assert len(messages) == 2
		assert messages[0]['role'] == 'system'
		assert messages[1]['role'] == 'user'
		sys_message = f"<s>[INST] <<SYS>>\n{messages[0]['content']}\n<</SYS>>\n\n"
		ins_message= f"{messages[1]['content']} [/INST]"
		prompt = sys_message + ins_message
		return prompt

	def load_llama_model(self):
		model_name_or_path = "/scratch/users/erjones/models/postprocessed_models/7B-chat"
		config = AutoConfig.from_pretrained(model_name_or_path)
		use_fast_tokenizer = "LlamaForCausalLM" not in config.architectures
		tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side="left")
		tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id
		tokenizer.bos_token_id = 1
		model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16,  device_map="auto")
		return model, tokenizer

	def extract_answer(self, llm_response):
		segments = llm_response.replace("[/INST]", "").split("The correct answer is: ")
		return (segments[-1])

	def	answer_batched_questions(self, batched_questions):
		modified_questions = [messages_to_prompt(generate_message(q)) for q in questions]
		
		answers = []
		batch_size = 96
		idx = 0

		pipeline = transformers.pipeline(
    		"text-generation",
    		model=self.model,
    		tokenizer=self.tokenizer,
    		torch_dtype = torch.float32, 
    		device_map = "auto"
		)

		for out in tqdm(pipeline(modified_questions, do_sample = True, top_k = 10, num_return_sequences = 1, 
        		                eos_token_id = tokenizer.eos_token_id, max_length = 200, batch_size = batch_size)):

			for sequence in out:
				answer = extract_answer(sequence['generated_text'])
				answers.append(answer)
    
				print("Question:", questions[idx], "Answer:", answer)    
				idx += 1

		return (answers)

class WildcardScorer:
	def __init__(self):
		self.wildcard_parser = WildcardParser()
		self.interacter = InteractLLaMA()

	def get_scores(self, questions):
		questions = questions[:2500]
		return (self.interacter.answer_batched_questions(questions))

	def evaluate_data(self, filter_toxic = False, filter_nonenglish = False, 
							filter_redacted = False ):

		''' Data in format (prompt, response, response_score, conversation_idx) '''
		self.prompt_ranking = []
		prompt_generator = self.wildcard_parser.get_prompt(filter_toxic, filter_nonenglish, filter_redacted)
		prompt_generator2 = self.wildcard_parser.get_prompt(filter_toxic, filter_nonenglish, filter_redacted)

		questions = []
		for conversation in prompt_generator2:
			for pair in conversation:
				questions.append(prefix_q + " (Prompt: " + pair[0] + ", Response: " + pair[1] + ")")

		question_scores = get_scores(questions)

		conversation_idx = 0
		pair_idx = 0
		for conversation in prompt_generator:
			for pair in conversation:
				self.prompt_ranking.append((pair[0], pair[1], question_scores[pair_idx], conversation_idx))
				pair_idx += 1

			conversation_idx += 1

		self.prompt_ranking.sort(key = lambda x : x[2])

	def get_best_n_prompt_responses(self, n):
		assert(n >= 0)
		return (self.prompt_ranking[:n])

if __name__ == "__main__":
	scorer = WildcardScorer()
	scorer.evaluate_data(False, True, False)

	result = scorer.get_best_n_prompt_responses(10)

	for i in range(len(result)):
		print("Element", i, ": ", result[i])
