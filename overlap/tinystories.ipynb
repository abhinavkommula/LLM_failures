{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e175e4a02d14ae1b0fc88516c29841b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c017ed00564d46d39ec4d00f02bdaef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55da21c7000543d2969d3f8788f33768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deab3911b8e749f0bca2ffcb40f89786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/jsteinhardt/akommula/.local/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2119719\n"
     ]
    }
   ],
   "source": [
    "from scrape_tinystories import TinyStoriesParser\n",
    "import random\n",
    "\n",
    "parser = TinyStoriesParser()\n",
    "stories = parser.get_stories()\n",
    "\n",
    "print(len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForCausalLM, GPTQConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "prefix_q = \"\"\"\n",
    "I will provide you with a short story. Please write a list of the protagonists of the story,\n",
    "along with a list of the antagonists. Make sure all characters belong to one or the other.\n",
    "Write your answer in the format: [The correct answer is: Protagonists: ..., Antagonists: ...].\n",
    "\"\"\"\n",
    "\n",
    "class InteractLLaMA:\n",
    "\tdef __init__(self):\n",
    "\t\tself.model, self.tokenizer = self.load_llama_model()\n",
    "\t\t\t\n",
    "\tdef generate_message(self, question):\n",
    "\t\tquestion = (prefix_q + question)\n",
    "\t\tformatted_question = question.replace(\"'\", '')\n",
    "\t\treturn ([{'role': 'system', 'content': ''}, \n",
    "\t\t\t\t {'role': 'user', 'content': (formatted_question)}])\n",
    "\n",
    "\tdef messages_to_prompt(self, messages):\n",
    "\t\tassert len(messages) == 2\n",
    "\t\tassert messages[0]['role'] == 'system'\n",
    "\t\tassert messages[1]['role'] == 'user'\n",
    "\t\tsys_message = f\"<s>[INST] <<SYS>>\\n{messages[0]['content']}\\n<</SYS>>\\n\\n\"\n",
    "\t\tins_message= f\"{messages[1]['content']} [/INST]\"\n",
    "\t\tprompt = sys_message + ins_message\n",
    "\t\treturn prompt\n",
    "\n",
    "\tdef load_llama_model(self):\n",
    "\t\tmodel_name_or_path = \"/scratch/users/erjones/models/postprocessed_models/7B-chat\"\n",
    "\t\tconfig = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\t\tuse_fast_tokenizer = \"LlamaForCausalLM\" not in config.architectures\n",
    "\t\ttokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\")\n",
    "\t\ttokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "\t\ttokenizer.bos_token_id = 1\n",
    "        \n",
    "\t\tgptq_config = GPTQConfig(bits=4, dataset=\"c4\", tokenizer = tokenizer)      \n",
    "\t\tmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, quantization_config=gptq_config, device_map=\"auto\")\n",
    "\t\treturn model, tokenizer\n",
    "\n",
    "\tdef extract_answer(self, llm_response, target):\n",
    "\t\tsegments = llm_response.replace(\"[/INST]\", \"\").split(\"[The correct answer is: \")\n",
    "\t\treturn (segments[-1])\n",
    "\n",
    "\tdef\tanswer_batched_questions(self, batched_questions):\n",
    "\t\tmodified_questions = [self.messages_to_prompt(self.generate_message(q)) for q in batched_questions]\n",
    "\t\t\n",
    "\t\tanswers = []\n",
    "\t\tbatch_size = 96\n",
    "\t\tidx = 0\n",
    "\n",
    "\t\tpipeline = transformers.pipeline(\n",
    "    \t\t\"text-generation\",\n",
    "    \t\tmodel=self.model,\n",
    "    \t\ttokenizer=self.tokenizer,\n",
    "    \t\ttorch_dtype = torch.float16, \n",
    "    \t\tdevice_map = \"auto\"\n",
    "\t\t)\n",
    "\n",
    "\t\tfor out in tqdm(pipeline(modified_questions, do_sample = True, top_k = 10, num_return_sequences = 1, \n",
    "        \t\t                eos_token_id = self.tokenizer.eos_token_id, max_length = 1000, batch_size = batch_size)):\n",
    "\n",
    "\t\t\tfor sequence in out:\n",
    "\t\t\t\tanswer = self.extract_answer(sequence['generated_text'])\n",
    "\t\t\t\tanswers.append(answer)    \n",
    "\n",
    "\t\treturn (answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(stories)\n",
    "\n",
    "short_stories = stories[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdc5f3ac7a84bd3a0aacd24c421a3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "/accounts/projects/jsteinhardt/akommula/.local/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c6d29545a444658b44af1bac775a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce571afec934beeadee8daf8e497cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interacter = InteractLLaMA()\n",
    "answers = interacter.answer_batched_questions(short_stories)\n",
    "\n",
    "print(answers[:100])\n",
    "\n",
    "'''\n",
    "erroneous = []\n",
    "story_id = 0\n",
    "for answer in answers:\n",
    "    protagonists = \n",
    "    antagonists = \n",
    "    \n",
    "    if bool(set(a) & set(b)):\n",
    "        print(\"Erroneous Overlapping. story_id:\", story_id)\n",
    "        erroneous.append(story_id)\n",
    "              \n",
    "    story_id += 1\n",
    "\n",
    "print(\"# errors:\", len(erroneous))\n",
    "    \n",
    "for err in erroneous:\n",
    "    print(short_stories[err])\n",
    "    print('=======================')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
